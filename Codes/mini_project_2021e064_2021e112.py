# -*- coding: utf-8 -*-
"""Mini_Project_2021E064_2021E112.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HdlfdaT9xm_4bezdFN8Q5SWBj1Vi835I

#### Step 1: Import Required Libraries
"""

import os
import numpy as np
import string
import pickle
from PIL import Image
from tqdm import tqdm
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model, load_model
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Add, Activation
import gc

# Paths
image_dir = '/content/drive/MyDrive/Images'
caption_file = '/content/captions.txt'
features_dir = 'features'
models_dir = 'models'
for d in [features_dir, models_dir]:
    os.makedirs(d, exist_ok=True)

"""#### Step 2: Load raw caption text into a Python dictionary

"""

def load_captions(filepath):
    with open(filepath, 'r') as f:
        lines = f.readlines()
    captions = dict()
    for line in lines:
        if ',' not in line: continue
        img, cap = line.strip().split(',', 1)
        captions.setdefault(img, []).append(cap)
    return captions

def clean_caption(caption):
    table = str.maketrans('', '', string.punctuation)
    words = caption.lower().split()
    words = [w.translate(table) for w in words]
    words = [w for w in words if w.isalpha() and len(w) > 1]
    return ' '.join(words)

def clean_all_captions(captions):
    for img, caps in captions.items():
        captions[img] = [clean_caption(c) for c in caps]
    return captions

def build_vocab(captions, min_freq=1):
    vocab = {}
    for caps in captions.values():
        for cap in caps:
            for word in cap.split():
                vocab[word] = vocab.get(word, 0) + 1
    return {w for w, c in vocab.items() if c >= min_freq}

# Load and clean captions
captions = load_captions(caption_file)
captions = clean_all_captions(captions)
vocab = build_vocab(captions)
print(f"Vocabulary size: {len(vocab)}")

"""####Step 3: Write cleaned caption data"""

def save_cleaned_captions(captions, filename):
    with open(filename, 'w') as f:
        for img, caps in captions.items():
            for cap in caps:
                f.write(f"{img}\t{cap}\n")
save_cleaned_captions(captions, 'cleaned_captions.txt')

"""####Step 4: Flatten all image captions into a single list"""

def all_captions_list(captions):
    return [c for caps in captions.values() for c in caps]

tokenizer = Tokenizer(oov_token='<unk>')
tokenizer.fit_on_texts(all_captions_list(captions))
vocab_size = len(tokenizer.word_index) + 1
pickle.dump(tokenizer, open(os.path.join(models_dir, 'tokenizer.pkl'), 'wb'))

max_length = max(len(c.split()) for c in all_captions_list(captions))
print(f"Max caption length: {max_length}")

"""#### Step 5: Build custom CNN for extracting image features"""

def build_custom_cnn():
    inputs = Input(shape=(224, 224, 3))

    # Initial layers with batch normalization
    x = Conv2D(64, 7, strides=2, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPooling2D(3, strides=2, padding='same')(x)

    # Block 1
    shortcut = x
    x = Conv2D(64, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(64, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Add()([x, shortcut])
    x = Activation('relu')(x)

    # Block 2
    shortcut = Conv2D(128, 1, strides=1, padding='same')(x)
    shortcut = BatchNormalization()(shortcut)

    x = Conv2D(128, 3, strides=1, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(128, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Add()([x, shortcut])
    x = Activation('relu')(x)

    # Block 3
    shortcut = Conv2D(256, 1, strides=2, padding='same')(x)
    shortcut = BatchNormalization()(shortcut)

    x = Conv2D(256, 3, strides=2, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(256, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Add()([x, shortcut])
    x = Activation('relu')(x)

    # Block 4
    shortcut = Conv2D(512, 1, strides=2, padding='same')(x)
    shortcut = BatchNormalization()(shortcut)

    x = Conv2D(512, 3, strides=2, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv2D(512, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Add()([x, shortcut])
    x = Activation('relu')(x)

    # Final layers
    x = Flatten()(x)
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)

    return tf.keras.Model(inputs, x)


# Build and verify
cnn_model = build_custom_cnn()
cnn_model.summary()

"""####Step 6: Traverse image folders and extract feature"""

def extract_features(directory, cnn_model):
    features = {}
    for img_name in tqdm(os.listdir(directory)):
        img_path = os.path.join(directory, img_name)
        try:
            img = Image.open(img_path).resize((224,224)).convert('RGB')
            img = np.array(img).astype('float32') / 255.0
            img = np.expand_dims(img, axis=0)
            feat = cnn_model.predict(img, verbose=0)
            features[img_name] = feat[0]
        except Exception as e:
            print(f"Error with {img_name}: {e}")
    with open(os.path.join(features_dir, 'features.pkl'), 'wb') as f:
        pickle.dump(features, f)
    return features

features = extract_features(image_dir, cnn_model)
print(f"Extracted features for {len(features)} images")

"""####Step 7: Create input–output sequence pairs from captions"""

def create_sequences(tokenizer, max_length, desc_list, feature, vocab_size):
    X1, X2, y = [], [], []
    for desc in desc_list:
        seq = tokenizer.texts_to_sequences([desc])[0]
        for i in range(1, len(seq)):
            in_seq, out_seq = seq[:i], seq[i]
            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
            X1.append(feature)
            X2.append(in_seq)
            y.append(out_seq)
    return np.array(X1), np.array(X2), np.array(y)

def data_generator(descriptions, features, tokenizer, max_length, vocab_size, batch_size=64):
    while True:
        X1, X2, y = [], [], []
        n = 0
        for img, desc_list in descriptions.items():
            if img not in features: continue
            feature = features[img]
            xi, xii, yi = create_sequences(tokenizer, max_length, desc_list, feature, vocab_size)
            for i in range(len(xi)):
                X1.append(xi[i])
                X2.append(xii[i])
                y.append(yi[i])
                n += 1
                if n == batch_size:
                    # Fix for newer TensorFlow versions - return dictionary instead of list
                    yield {'input_1': np.array(X1), 'input_2': np.array(X2)}, np.array(y)
                    X1, X2, y = [], [], []
                    n = 0

"""####Step 8: Combine CNN features with LSTM layers to define the captioning model"""

def build_caption_model(vocab_size, max_length):
    # Image feature branch
    inputs1 = Input(shape=(512,), name='input_1')
    fe1 = Dropout(0.4)(inputs1)
    fe2 = Dense(128, activation='relu')(fe1)

    # Caption branch
    inputs2 = Input(shape=(max_length,), name='input_2')
    se1 = Embedding(vocab_size, 512, mask_zero=False)(inputs2)
    se2 = Dropout(0.4)(se1)
    se3 = LSTM(128, implementation=1)(se2)  # implementation=1 to avoid cuDNN issues

    # Decoder
    decoder1 = Add()((fe2, se3))
    decoder2 = Dense(128, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    return model

caption_model = build_caption_model(vocab_size, max_length)
caption_model.summary()

"""####Step 9: Split dataset into training and validation sets and train the model"""

# Split images for training/validation (e.g., 90/10 split)
all_imgs = list(captions.keys())
np.random.shuffle(all_imgs)
split = int(0.9 * len(all_imgs))
train_imgs = all_imgs[:split]
val_imgs = all_imgs[split:]

train_desc = {k: captions[k] for k in train_imgs}
val_desc = {k: captions[k] for k in val_imgs}
train_feats = {k: features[k] for k in train_imgs if k in features}
val_feats = {k: features[k] for k in val_imgs if k in features}

# Make sure your model is compiled with accuracy metric
caption_model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

train_gen = data_generator(train_desc, train_feats, tokenizer, max_length, vocab_size, batch_size=64)
val_gen = data_generator(val_desc, val_feats, tokenizer, max_length, vocab_size, batch_size=64)

# Calculate steps
steps = sum(len(captions[k]) for k in train_imgs if k in features) // 64
val_steps = sum(len(captions[k]) for k in val_imgs if k in features) // 64

# Custom callback to display and log accuracy for each epoch
class AccuracyMonitor(tf.keras.callbacks.Callback):
    def __init__(self):
        super().__init__()
        self.train_accuracies = []
        self.val_accuracies = []

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        train_acc = logs.get('accuracy', 0)
        val_acc = logs.get('val_accuracy', 0)

        self.train_accuracies.append(train_acc)
        self.val_accuracies.append(val_acc)

        print(f"\nEpoch {epoch+1} Accuracy Metrics:")
        print(f"Training Accuracy: {train_acc:.4f}")
        print(f"Validation Accuracy: {val_acc:.4f}")

# Memory monitoring callback
class MemoryMonitor(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        gc.collect()
        print("Memory cleaned after epoch")

# Create a callback to save model in both formats
class SaveMultipleFormats(tf.keras.callbacks.Callback):
    def on_train_end(self, logs=None):
        # Save in .h5 format (already done by ModelCheckpoint, but ensuring latest is saved)
        self.model.save(os.path.join(models_dir, 'caption_model.h5'))

        # Also save in .keras format (newer format with better compatibility)
        self.model.save(os.path.join(models_dir, 'caption_model.keras'))

        print("\nModel saved in both .h5 and .keras formats")

# Initialize the accuracy monitor
accuracy_monitor = AccuracyMonitor()

# Train the model with all callbacks
history = caption_model.fit(
    train_gen,
    steps_per_epoch=steps,
    epochs=20,
    validation_data=val_gen,
    validation_steps=val_steps,
    callbacks=[
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(models_dir, 'caption_model.h5'),
            save_best_only=True,
            monitor='val_loss'
        ),
        tf.keras.callbacks.EarlyStopping(patience=15),
        MemoryMonitor(),
        accuracy_monitor,
        SaveMultipleFormats()
    ],
    verbose=1  # Show progress bar
)

# Optional: Plot accuracy over epochs
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(accuracy_monitor.train_accuracies, label='Training Accuracy')
plt.plot(accuracy_monitor.val_accuracies, label='Validation Accuracy')
plt.title('Model Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig(os.path.join(models_dir, 'accuracy_plot.png'))
plt.show()

print("Training completed with accuracy tracking and models saved in multiple formats")

"""####Step 10: Train the model using k‑fold cross‑validation"""

def train_with_cross_validation(captions, features, tokenizer, max_length, vocab_size, k_folds=5, epochs=10, batch_size=64):
    """
    Train the model with k-fold cross-validation
    """
    # Get all image IDs
    all_imgs = list(captions.keys())
    np.random.shuffle(all_imgs)

    # Split into k folds
    fold_size = len(all_imgs) // k_folds
    folds = [all_imgs[i:i + fold_size] for i in range(0, len(all_imgs), fold_size)]

    # Ensure we have exactly k folds
    while len(folds) > k_folds:
        last_fold = folds.pop()
        folds[-1].extend(last_fold)

    # Store validation scores
    val_scores = []

    # Train k models
    for i in range(k_folds):
        print(f"\nTraining fold {i+1}/{k_folds}")

        # Get validation set
        val_imgs = folds[i]

        # Get training set (all other folds)
        train_imgs = []
        for j in range(k_folds):
            if j != i:
                train_imgs.extend(folds[j])

        # Create training and validation descriptions
        train_desc = {k: captions[k] for k in train_imgs if k in captions}
        val_desc = {k: captions[k] for k in val_imgs if k in captions}

        # Create features for training and validation sets
        train_feats = {k: features[k] for k in train_imgs if k in features}
        val_feats = {k: features[k] for k in val_imgs if k in features}

        # Create data generators
        train_gen = data_generator(train_desc, train_feats, tokenizer, max_length, vocab_size, batch_size)
        val_gen = data_generator(val_desc, val_feats, tokenizer, max_length, vocab_size, batch_size)

        # Calculate steps
        train_steps = sum(len(captions[k]) for k in train_imgs if k in features) // batch_size
        val_steps = sum(len(captions[k]) for k in val_imgs if k in features) // batch_size

        # Ensure at least one step
        train_steps = max(1, train_steps)
        val_steps = max(1, val_steps)

        # Build model
        model = build_caption_model(vocab_size, max_length)

        # Setup callbacks
        checkpoint = tf.keras.callbacks.ModelCheckpoint(
            os.path.join(models_dir, f'model_fold_{i+1}.h5'),
            monitor='val_loss',
            verbose=1,
            save_best_only=True
        )
        early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, verbose=1)

        # Train the model
        history = model.fit(
            train_gen,
            steps_per_epoch=train_steps,
            epochs=epochs,
            validation_data=val_gen,
            validation_steps=val_steps,
            callbacks=[checkpoint, early_stopping, MemoryMonitor()],
            verbose=1
        )

        # Save validation score
        best_val_loss = min(history.history['val_loss'])
        val_scores.append(best_val_loss)

        # Plot training history
        plt.figure(figsize=(12, 6))
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title(f'Training and Validation Loss - Fold {i+1}')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.savefig(os.path.join(models_dir, f'loss_fold_{i+1}.png'))
        plt.close()

        # Clean up memory
        gc.collect()

    # Print cross-validation results
    print("\nCross-Validation Results:")
    for i, score in enumerate(val_scores):
        print(f"Fold {i+1}: Loss = {score:.4f}")
    print(f"Average Loss: {np.mean(val_scores):.4f}")

    return val_scores

  # Memory monitoring callback
class MemoryMonitor(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        gc.collect()
        print("Memory cleaned after epoch")

# perform cross-validation
print("\nPerforming cross-validation...")
val_scores = train_with_cross_validation(captions, features, tokenizer, max_length, vocab_size)

"""####Step 11: Set up grid search for tuning hyperparameters

"""

def hyperparameter_tuning(captions, features, tokenizer, max_length, vocab_size):
    """
    Perform hyperparameter tuning to find the best model configuration
    """
    # Define hyperparameter grid
    embedding_dims = [128, 256, 512]
    lstm_units = [128, 256, 512]
    dropout_rates = [0.3, 0.4, 0.5]
    batch_sizes = [32, 64, 128]

    # Get a subset of data for tuning (to save time)
    all_imgs = list(captions.keys())
    np.random.shuffle(all_imgs)
    tuning_img_ids = all_imgs[:min(1000, len(all_imgs))]

    # Split into train and validation
    train_size = int(0.8 * len(tuning_img_ids))
    train_imgs = tuning_img_ids[:train_size]
    val_imgs = tuning_img_ids[train_size:]

    # Create training and validation descriptions
    train_desc = {k: captions[k] for k in train_imgs if k in captions}
    val_desc = {k: captions[k] for k in val_imgs if k in captions}

    # Create features for training and validation sets
    train_feats = {k: features[k] for k in train_imgs if k in features}
    val_feats = {k: features[k] for k in val_imgs if k in features}

    # Store results
    results = []

    # Try different combinations
    for embedding_dim in embedding_dims:
        for lstm_unit in lstm_units:
            for dropout_rate in dropout_rates:
                for batch_size in batch_sizes:
                    print(f"\nTrying: embedding_dim={embedding_dim}, lstm_units={lstm_unit}, dropout_rate={dropout_rate}, batch_size={batch_size}")

                    # Build model with current hyperparameters
                    inputs1 = Input(shape=(512,), name='input_1')
                    fe1 = Dropout(dropout_rate)(inputs1)
                    fe2 = Dense(lstm_unit, activation='relu')(fe1)

                    inputs2 = Input(shape=(max_length,), name='input_2')
                    se1 = Embedding(vocab_size, embedding_dim, mask_zero=False)(inputs2)
                    se2 = Dropout(dropout_rate)(se1)
                    se3 = LSTM(lstm_unit, implementation=1)(se2)

                    decoder1 = Add()((fe2, se3))
                    decoder2 = Dense(lstm_unit, activation='relu')(decoder1)
                    outputs = Dense(vocab_size, activation='softmax')(decoder2)

                    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
                    model.compile(loss='categorical_crossentropy', optimizer='adam')

                    # Create data generators
                    train_gen = data_generator(train_desc, train_feats, tokenizer, max_length, vocab_size, batch_size)
                    val_gen = data_generator(val_desc, val_feats, tokenizer, max_length, vocab_size, batch_size)

                    # Calculate steps
                    train_steps = sum(len(captions[k]) for k in train_imgs if k in features) // batch_size
                    val_steps = sum(len(captions[k]) for k in val_imgs if k in features) // batch_size

                    # Ensure at least one step
                    train_steps = max(1, train_steps)
                    val_steps = max(1, val_steps)

                    # Train for a few epochs
                    history = model.fit(
                        train_gen,
                        steps_per_epoch=train_steps,
                        epochs=5,  # Just a few epochs for tuning
                        validation_data=val_gen,
                        validation_steps=val_steps,
                        callbacks=[MemoryMonitor()],
                        verbose=1
                    )

                    # Get validation loss
                    val_loss = history.history['val_loss'][-1]

                    # Store result
                    results.append({
                        'embedding_dim': embedding_dim,
                        'lstm_units': lstm_unit,
                        'dropout_rate': dropout_rate,
                        'batch_size': batch_size,
                        'val_loss': val_loss
                    })

                    # Print current result
                    print(f"Validation Loss: {val_loss:.4f}")

                    # Clean up memory
                    del model
                    gc.collect()

    # Sort results by validation loss
    results.sort(key=lambda x: x['val_loss'])

    # Print best configurations
    print("\nBest Hyperparameter Configurations:")
    for i, result in enumerate(results[:5]):
        print(f"Rank {i+1}:")
        print(f"  Embedding Dim: {result['embedding_dim']}")
        print(f"  LSTM Units: {result['lstm_units']}")
        print(f"  Dropout Rate: {result['dropout_rate']}")
        print(f"  Batch Size: {result['batch_size']}")
        print(f"  Validation Loss: {result['val_loss']:.4f}")

    return results

# perform hyperparameter tuning
print("\nPerforming hyperparameter tuning...")
tuning_results = hyperparameter_tuning(captions, features, tokenizer, max_length, vocab_size)
best_params = tuning_results[0]
print(f"\nBest parameters found: {best_params}")

"""####Step 12: Train final model on full data using best parameters"""

def train_final_model(descriptions, features, tokenizer, max_length, vocab_size, best_params, epochs=30):
    """
    Train the final model with the best hyperparameters
    """
    # Extract best parameters
    embedding_dim = best_params['embedding_dim']
    lstm_units = best_params['lstm_units']
    dropout_rate = best_params['dropout_rate']
    batch_size = best_params['batch_size']

    # Build model with best hyperparameters
    inputs1 = Input(shape=(512,), name='input_1')
    fe1 = Dropout(dropout_rate)(inputs1)
    fe2 = Dense(lstm_units, activation='relu')(fe1)

    inputs2 = Input(shape=(max_length,), name='input_2')
    se1 = Embedding(vocab_size, embedding_dim, mask_zero=False)(inputs2)
    se2 = Dropout(dropout_rate)(se1)
    se3 = LSTM(lstm_units)(se2)

    decoder1 = Add()([fe2, se3])
    decoder2 = Dense(lstm_units, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # Filter to only include images that have features
    valid_img_ids = [img_id for img_id in captions.keys() if img_id in features]
    valid_descriptions = {img_id: captions[img_id] for img_id in valid_img_ids}

    print(f"Training final model with {len(valid_descriptions)} images")

    # Create data generator for all data
    data_gen = data_generator(valid_descriptions, features, tokenizer, max_length, vocab_size, batch_size)

    # Calculate steps
    steps = sum(len(captions_list) for captions_list in captions.values()) // batch_size
    steps = max(1, steps)

    # Setup callbacks
    checkpoint = ModelCheckpoint('models/best_model.h5', monitor='loss', verbose=1, save_best_only=True)

    # Train the model
    history = model.fit(
        data_gen,
        steps_per_epoch=steps,
        epochs=epochs,
        callbacks=[checkpoint],
        verbose=1
    )

    # Save the final model
    model.save('models/final_model.h5')

    # Plot training history
    plt.figure(figsize=(12, 6))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.title('Training Loss - Final Model')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig('models/final_model_loss.png')
    plt.close()

    return model

# Train final model
final_model = train_final_model(captions, features, tokenizer, max_length, vocab_size, best_params)

"""####Step 13: Generate captions for new images with the trained model"""

def generate_caption(image_path, model, tokenizer, max_length, cnn_model):
    """
    Generate a caption for a given image
    """
    # Get original filename
    filename = os.path.basename(image_path)

    # Load and preprocess the image
    img = load_img(image_path, target_size=(224, 224))
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = img_array / 255.0  # Normalize

    # Extract features
    feature = cnn_model.predict(img_array, verbose=0)

    # Generate caption
    in_text = 'startseq'

    # Generate caption word by word
    for i in range(max_length):
        # Encode the current input sequence
        sequence = tokenizer.texts_to_sequences([in_text])[0]

        # Pad the sequence
        sequence = pad_sequences([sequence], maxlen=max_length)

        # Predict next word
        yhat = model.predict([feature, sequence], verbose=0)

        # Get index with highest probability
        yhat = np.argmax(yhat)

        # Convert index to word
        word = ''
        for w, index in tokenizer.word_index.items():
            if index == yhat:
                word = w
                break

        # Stop if we predict the end of sequence
        if word == 'endseq':
            break

        # Append word to the caption
        in_text += ' ' + word

    # Remove start and end tokens
    caption = in_text.replace('startseq', '').replace('endseq', '').strip()

    return filename, caption

# Test the caption generator
def test_caption_generator(test_image_path, model, tokenizer, max_length, cnn_model):
    """
    Test the caption generator on a sample image
    """
    # Generate caption
    filename, caption = generate_caption(test_image_path, model, tokenizer, max_length, cnn_model)

    # Display image and caption
    img = Image.open(test_image_path)
    plt.figure(figsize=(10, 8))
    plt.imshow(img)
    plt.title(f"Filename: {filename}\nCaption: {caption}")
    plt.axis('off')
    plt.show()

    return filename, caption

"""####Step 14: Convert integer tokens back into human‑readable words"""

def word_for_id(integer, tokenizer):
    for word, idx in tokenizer.word_index.items():
        if idx == integer:
            return word
    return None

def generate_caption(model, tokenizer, photo, max_length):
    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = model.predict({'input_1': photo.reshape(1,-1), 'input_2': sequence}, verbose=0)
        yhat = np.argmax(yhat)
        word = word_for_id(yhat, tokenizer)
        if word is None or word == 'endseq':
            break
        in_text += ' ' + word
    return in_text.replace('startseq', '').strip()

"""####Step 15: Load the model and evaluate by generating captions"""

# Define paths
image_dir = '/content/drive/MyDrive/Images'
models_dir = 'models'

# Load the tokenizer
with open(os.path.join(models_dir, 'tokenizer.pkl'), 'rb') as handle:
    tokenizer = pickle.load(handle)

# Load the CNN model
# cnn_model = load_model(os.path.join(models_dir, 'cnn_model.h5'))

# First try to load the final model
caption_model = load_model(os.path.join(models_dir, 'final_model.h5'))

# Load the caption model
caption_model = load_model(os.path.join(models_dir, 'caption_model.h5'))

# Load captions dictionary (if available)
try:
    with open('cleaned_captions.txt', 'r') as f:
        lines = f.readlines()

    captions = {}
    for line in lines:
        parts = line.strip().split('\t')
        if len(parts) == 2:
            img, cap = parts
            if img not in captions:
                captions[img] = []
            captions[img].append(cap)
except:
    # If cleaned_captions.txt is not available, try loading from the original file
    with open('/content/captions.txt', 'r') as f:
        lines = f.readlines()

    captions = {}
    for line in lines:
        if ',' not in line: continue
        img, cap = line.strip().split(',', 1)
        if img not in captions:
            captions[img] = []
        captions[img].append(cap)

# Get all image files from the directory
image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]

# Select a random image
random_img = random.choice(image_files)
img_path = os.path.join(image_dir, random_img)

# Function to generate caption
def generate_caption(model, tokenizer, photo, max_length):
    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)

        # Handle different model input formats
        try:
            yhat = model.predict({'input_1': photo.reshape(1,-1), 'input_2': sequence}, verbose=0)
        except:
            yhat = model.predict([photo.reshape(1,-1), sequence], verbose=0)

        yhat = np.argmax(yhat)

        # Convert index to word
        word = None
        for w, idx in tokenizer.word_index.items():
            if idx == yhat:
                word = w
                break

        # Stop if we predict the end of sequence or can't find the word
        if word is None or word == 'endseq':
            break

        in_text += ' ' + word

    return in_text.replace('startseq', '').strip()

# Load and preprocess the image
img = Image.open(img_path).resize((224, 224)).convert('RGB')
img_array = np.array(img).astype('float32') / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Extract features
features = cnn_model.predict(img_array, verbose=0)

# Get the maximum caption length
max_length = 32

# Generate caption
generated_caption = generate_caption(caption_model, tokenizer, features[0], max_length)

# Get actual captions if available
img_name = os.path.basename(img_path)
actual_captions = captions.get(img_name, ["No actual caption available"])

# Create a figure with the image and captions
plt.figure(figsize=(10, 8))
plt.imshow(img)
plt.axis('off')

# Create title with generated and actual captions
title = f"Generated: {generated_caption}\n\nActual: {actual_captions[0]}"
plt.title(title, fontsize=12, loc='left', wrap=True)

# Add a border for better visibility
plt.gca().spines['top'].set_visible(True)
plt.gca().spines['right'].set_visible(True)
plt.gca().spines['bottom'].set_visible(True)
plt.gca().spines['left'].set_visible(True)

# Save the visualization
output_filename = f"caption_test_result_{os.path.splitext(img_name)[0]}.png"
plt.savefig(output_filename, bbox_inches='tight', dpi=300)
plt.show()

# Print information
print(f"Image: {img_name}")
print(f"Generated Caption: {generated_caption}")
print(f"Actual Caption(s):")
for i, cap in enumerate(actual_captions):
    print(f"  {i+1}. {cap}")
print(f"\nOutput saved as: {output_filename}")